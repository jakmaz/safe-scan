# SafeScan - Content Moderation and Violation Detection

![image](https://github.com/user-attachments/assets/9dcc935f-15c8-4fa8-8222-f3622162e686)


SafeScan is an advanced text moderation tool that scans text for potential violations and harmful content. The application uses OpenAI's content moderation models to check input against various harmful categories such as `sexual`, `harassment`, `hate`, `self-harm`, `violence`, and more. Ensure your content complies with community standards in real-time.

## Features

- **Real-time Text Scanning**: Instantly analyze user input for potential content violations.
- **Categories**: Scan for violations in categories like `sexual`, `harassment`, `hate`, `self-harm`, and more.
- **Flagging**: Text flagged as harmful is clearly indicated.
